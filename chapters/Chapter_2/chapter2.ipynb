{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tour of Traditional NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The term natural language processing is a more specific term referring to the sub-field of computer science that deals with methods to analyze, model, and understand human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "- Natural language processing (evolved from computational linguistics) uses methods from various disciplines, such as computer science, artificial intelligence, linguistics, and data science, to enable computers to understand human language in both written and verbal forms. \n",
    "\n",
    "### Diffrence between NLP and Computational Linguistisc?\n",
    "\n",
    "> Natural language processing emphasizes its use of machine learning and deep learning techniques to complete tasks, like language translation or question answering.\n",
    " \n",
    "\n",
    "> While computational linguistics has more of a focus on aspects of language, such as syntax, semantics, and grammatical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLP vs NLU VS NLG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While natural language processing (NLP), natural language understanding (NLU), and natural language generation (NLG) are all related topics, they are distinct ones.\n",
    "\n",
    "- At a high level, NLU and NLG are just components of NLP.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"NLP_NLU.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Natural Language Understanding** (When you want to understand the meaning of a sentence)\n",
    "\n",
    "\n",
    "- Natural language understanding is a subset of natural language processing, which uses syntactic and semantic analysis of text and speech to determine the meaning of a sentence.\n",
    "\n",
    "\n",
    "Example 1:\n",
    "\n",
    "- Alice is swimming against the current.\n",
    "\n",
    "- The current version of the report is in the folder.\n",
    "\n",
    "\n",
    "Example 2:\n",
    "\n",
    "- I will give you a ring tomorrow.\n",
    "\n",
    "- The ring is in the folder.\n",
    "\n",
    "\n",
    "Example 3:\n",
    "\n",
    "\n",
    "- The profits increases by 10%.\n",
    "\n",
    "- The pains increase day by day.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Natural Language Generation** (When computers writes language)\n",
    "\n",
    "\n",
    "- While natural language understanding focuses on computer reading comprehension, natural language generation enables computers to write. \n",
    "\n",
    "- NLG is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services.\n",
    "\n",
    "- NLG tasks include: \n",
    "  \n",
    "  - generating text, \n",
    "  \n",
    "  - generating speech, \n",
    "  \n",
    "  - and generating images \n",
    "  \n",
    "  - and videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](npl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Heuristics-based NLP (Rule-based NLP)\n",
    "\n",
    "- Machine Learning NLP\n",
    "\n",
    "- Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics-based NLP Examples:\n",
    "\n",
    "- Dictionary-based sentiment analysis (Lexicon-based SA)\n",
    "\n",
    "- WordNet for lexical relations\n",
    "\n",
    "- Regular Expressions\n",
    "\n",
    "- Context-free grammar\n",
    "\n",
    "### Strengths:\n",
    "\n",
    "- Rules based on domain-specific knowledge can efficiently reduce the mistakes that are sometimes very expensive.\n",
    "\n",
    "### Dis\n",
    "\n",
    "- Manually curation of feuatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning for NLP\n",
    "\n",
    "\n",
    "### Common methods for machine learning:\n",
    "\n",
    "- Naive Bayes\n",
    "  \n",
    "- Logistic Regression\n",
    "\n",
    "- Support Vector Machine\n",
    "\n",
    "- Hidden Markov Model\n",
    "\n",
    "- Conditional Random Field\n",
    "\n",
    "\n",
    "### Three common steps for machine learning\n",
    "\n",
    "- Extracting features from texts\n",
    "\n",
    "- Using the feature representation to learn a model\n",
    "\n",
    "- Evaluating and improving the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP\n",
    "\n",
    "Convolutional Neural Network (CNN)\n",
    "  \n",
    "Sequence Models\n",
    "  \n",
    "  - Recurrent Neural Network (RNN)\n",
    "  \n",
    "  - Long-Term Short-Term Memory (LSTM)\n",
    "\n",
    "\n",
    "Strengths of Sequence Models\n",
    "\n",
    "- It reflects the fact that a sentence in language flows from one direction to another.\n",
    "\n",
    "- The model can progressively read an input text from one end to another.\n",
    "\n",
    "- The model have neural units capable of remembering what it has processed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning\n",
    "\n",
    "- It is a technique in AI where the knowledge gained while solving one problem is applied to a different but related problem.\n",
    "\n",
    "- We can use unsupervised methods to train a transformer-based model for predicting a part of a sentence given the rest of the content.\n",
    "\n",
    "- This model can encode high-level nuances of the language, which can be applied to other relevant downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers\n",
    "\n",
    "- The state-of-the-art model in major NLP tasks\n",
    "  \n",
    "- It models the textual context in a non-sequential manner.\n",
    "\n",
    "- Given a word in the input, the model looks at all the words around it and represent each word with respect to its context. This is referred to as self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora, Tokens, and Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All NLP methods, be they classic or modern, begin with a text dataset, also called a corpus (plural: corpora)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A corpus is a representative sample of actual language production within a meaningful context and with a general purpose. \n",
    "\n",
    "\n",
    "> A dataset is a representative sample of a specific linguistic phenomenon in a restricted context and with annotations that relate to a specific research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both terms are use interchangebly in the NLP literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"corpus_dataset.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata could be any auxiliary piece of information associated with the text, like identifiers, labels, and timestamps In machine learning parlance, the text along with its metadata is called an instance or data point. \n",
    "\n",
    "- We freely interchange the terms corpus and dataset throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"corpus.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A General NLP Pipeline\n",
    "\n",
    "Varations of the NLP Pipelines\n",
    "\n",
    "- The process may not always be linear.\n",
    "  \n",
    "- There are loops in between.\n",
    "\n",
    "- These procedures may depend on specific task at hand.\n",
    "\n",
    "\n",
    "\n",
    "![](./nlp_pipe_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "- Ideal Setting: We have everything needed.\n",
    "\n",
    "- Labels and Annotations\n",
    "\n",
    "- Very often we are dealing with less-than-ideal scenario (scrape the data, public datasets)\n",
    "\n",
    "- Initial datasets with limited annotations/labels (one solution: data augmentation)\n",
    "\n",
    "**Data augmentation :** It is a technique to exploit language properties to create texts that are syntactically similar to the source text data.\n",
    "Types of strategies:\n",
    "\n",
    " - synonym replacement\n",
    " \n",
    " - Related word replacement (based on association metrics)\n",
    " \n",
    " - Back translation\n",
    " \n",
    " - Replacing entities\n",
    " \n",
    " - Adding noise to data (e.g. spelling errors, random words) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Relevant vs. irrelevant information\n",
    "\n",
    "- non-textual information\n",
    "\n",
    "- markup\n",
    "\n",
    "- metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ\n",
      "b'I feel really \\xf0\\x9f\\x98\\xa1. GOGOGO!! \\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa  \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f\\xa4\\xa3 \\xc8\\x80\\xc3\\x86\\xc4\\x8e\\xc7\\xa6\\xc6\\x93'\n"
     ]
    }
   ],
   "source": [
    "text = 'I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ'\n",
    "print(text)\n",
    "text2 = text.encode('utf-8') # encode the strings in bytes\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel really . GOGOGO!!    ADG'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Preliminaries\n",
    "\n",
    "- Sentence segmentation\n",
    "\n",
    "- Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The process of breaking a text down into tokens is called tokenization. For example, there are six tokens in the Esperanto sentence “Maria frapis la verda sorĉistino.” \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tokenization.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tokenization_general.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"./tokenization_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./tokenization_example.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is an interpreted, high-level and general-purpose programming language.\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.']\n",
      "Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
      "['Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.']\n",
      "Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
      "['Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Frequent preprocessing\n",
    "\n",
    "- Stopword removal\n",
    "\n",
    "- Stemming and/or lemmatization\n",
    "\n",
    "- Digits/Punctuaions removal\n",
    "\n",
    "- Case normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords, punctuations, digits¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shmuhammad/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"O'Neil\", 'works', 'at', 'Wonderland', ',', 'located', 'at', '245', 'Goleta', 'Avenue', ',', 'CA.', ',', '74208', '.']\n",
      "Mr.\n",
      "John\n",
      "O'Neil\n",
      "works\n",
      "Wonderland\n",
      "located\n",
      "Goleta\n",
      "Avenue\n",
      "CA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas and Stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lemmas are root forms of words. Consider the verb fly. It can be inflected into many different words —flow, flew, flies, flown, flowing, and so on—and **fly** is the lemma for all of these seemingly different words. \n",
    "\n",
    "#### Stemming Algorithm\n",
    "\n",
    "- Stemming Algorithm:  algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. \n",
    "\n",
    "- This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations\n",
    "\n",
    "#### Lemmatization Algorithm\n",
    "\n",
    "- Lemmatization algorithm, on the other hand, takes into consideration the morphological analysis of the words. \n",
    "\n",
    "- To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma.\n",
    "\n",
    "\n",
    "Sometimes, it might be useful to reduce the tokens to their lemmas to keep the dimensionality of the vector representation low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./lemma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Stemming is the poor­man’s lemmatization. It involves the use of handcrafted rules to strip endings  of words to reduce them to a common form called stems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular stemmers often implemented in open source packages include the Porter and Snowball stemmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'revolut', 'better']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "revolution\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Reminders for Preprocessing\n",
    "\n",
    "- Not all steps are necessary\n",
    "\n",
    "- These steps are NOT sequential\n",
    "\n",
    "- These steps are task-dependent\n",
    "\n",
    "Goals\n",
    "\n",
    "- Text Normalization\n",
    "\n",
    "- Text Tokenization\n",
    "\n",
    "- Text Enrichment/Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The process of breaking a text down into tokens is called tokenization. For example, there are six tokens in the Esperanto sentence “Maria frapis la verda sorĉistino.” \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank tokenizer : Let's, go, to, N.Y., \n",
      "Default tokenizer : Let, 's, go, to, N.Y., "
     ]
    }
   ],
   "source": [
    "# Construction 1\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Creating a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(\"Let's go to N.Y.\")\n",
    "print(\"Blank tokenizer\",end=\" : \")\n",
    "for token in tokens:\n",
    "    print(token,end=', ')\n",
    " \n",
    "# Construction 2\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Creating a Tokenizer with the default settings for English\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(\"Let's go to N.Y.\")\n",
    "print(\"\\nDefault tokenizer\",end=' : ')\n",
    "for token in tokens:\n",
    "    print(token,end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization can become more complicated than simply splitting text based on nonalphanumeric characters, \n",
    "\n",
    "- For agglutinative languages like Turkish, splitting on whitespace and punctuation might not be sufficient and more specialized techniques might be needed (chap 5 and 6).\n",
    "\n",
    "- It may be possible to entirely circumvent the issue of tokenization in some neural network models by representing text as a stream of bytes; this becomes very important for agglutinative languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](chapters/Chapter_2/aglunative_hungarian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feauture Engineering\n",
    "\n",
    "- It refers to a process to feed the extracted and preprocessed texts into a machine-learning algorithm.\n",
    "\n",
    "- It aims at capturing the characteristics of the text into a numeric vector that can be understood by the ML algorithms. \n",
    "  \n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., text representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Classical ML\n",
    "\n",
    "- Word-based frequency lists\n",
    "\n",
    "- Bag-of-words representations\n",
    "\n",
    "- Domain-specific word frequency lists\n",
    "\n",
    "- Handcrafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams, Bigrams, Trigrams, ..., N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N­grams are fixed­length (n) consecutive token sequences occurring in the text. A bigram has two tokens, a unigram one\n",
    "\n",
    "- Generating n­grams from a text is straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "My_text = 'Jack is very good in mathematics but he is not that much good in science'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  ['Jack', 'is', 'very', 'good', 'in', 'mathematics', 'but', 'he', 'is', 'not', 'that', 'much', 'good', 'in', 'science'] \n",
      "\n",
      "2-gram  ['Jack is', 'is very', 'very good', 'good in', 'in mathematics', 'mathematics but', 'but he', 'he is', 'is not', 'not that', 'that much', 'much good', 'good in', 'in science'] \n",
      "\n",
      "3-gram:  ['Jack is very', 'is very good', 'very good in', 'good in mathematics', 'in mathematics but', 'mathematics but he', 'but he is', 'he is not', 'is not that', 'not that much', 'that much good', 'much good in', 'good in science'] \n",
      "\n",
      "4-gram:  ['Jack is very good', 'is very good in', 'very good in mathematics', 'good in mathematics but', 'in mathematics but he', 'mathematics but he is', 'but he is not', 'he is not that', 'is not that much', 'not that much good', 'that much good in', 'much good in science'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram \", extract_ngrams(My_text, 1), '\\n')\n",
    "print(\"2-gram \", extract_ngrams(My_text, 2), '\\n')\n",
    "print(\"3-gram: \", extract_ngrams(My_text, 3), '\\n')\n",
    "print(\"4-gram: \", extract_ngrams(My_text, 4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "\n",
    "- The DL model is capable of learning features from the texts (e.g., embeddings)\n",
    "\n",
    "-  The price is that the model is often less interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## From Simple to Complex\n",
    "\n",
    "Start with heuristics or rules\n",
    "Experiment with different ML models\n",
    "\n",
    "- From heuristics to features\n",
    "  \n",
    "- From manual annotation to automatic extraction\n",
    "\n",
    "Find the most optimal model\n",
    "\n",
    "- Ensemble and stacking\n",
    "  \n",
    "- Redo feature engineering\n",
    "  \n",
    "- Transfer learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. NLTK Tutorials: https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](chapters/Chapter_2/aglunativelanguage.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d203a7fbe37afbb990fedfc21c321928443618f3d7b991e0237ff71906aa031f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
